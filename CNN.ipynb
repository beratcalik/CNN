{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae24101d-a41f-4e07-9a93-94d418fbb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from sklearn import svm, ensemble\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a7df85-439b-42d8-92d9-9ebec612360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc9a1dc-b4bb-4f2e-a511-7d269678d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loaders\n",
    "# MNIST for Model1 and Model2\n",
    "tf_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_mnist = datasets.MNIST(root='./data', train=True, download=True, transform=tf_mnist)\n",
    "test_mnist = datasets.MNIST(root='./data', train=False, download=True, transform=tf_mnist)\n",
    "loader_train_mnist = DataLoader(train_mnist, batch_size=64, shuffle=True)\n",
    "loader_test_mnist = DataLoader(test_mnist, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180313b3-e81b-4b1d-96c6-2dafd20310ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 for Model3\n",
    "# Resize to 224x224 for pretrained models\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "tf_cifar = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "train_cifar = datasets.CIFAR10(root='./data', train=True, download=True, transform=tf_cifar)\n",
    "test_cifar = datasets.CIFAR10(root='./data', train=False, download=True, transform=tf_cifar)\n",
    "loader_train_cifar = DataLoader(train_cifar, batch_size=64, shuffle=True)\n",
    "loader_test_cifar = DataLoader(test_cifar, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26cbc994-1453-4611-b623-ea58d0465040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model 1: LeNet-5-like CNN\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c1c9a2-5fc0-4ffb-91b3-96b1e0afdf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model 2: Improved LeNet with BatchNorm and Dropout\n",
    "class ImprovedLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedLeNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c60e86-142d-424d-b56a-7c9c73acb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Training and Evaluation\n",
    "\n",
    "def train_model(model, loader_train, criterion, optimizer, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in loader_train:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        epoch_loss = running_loss / len(loader_train.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, loader_test):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_test:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print('Confusion Matrix:\\n', cm)\n",
    "    return acc, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb2d65a-74a2-4e2e-948a-a6425b31e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berat\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\berat\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to C:\\Users\\berat/.cache\\torch\\hub\\checkpoints\\vgg11-8a719046.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 507M/507M [06:48<00:00, 1.30MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. Model 3: Pretrained VGG11 on CIFAR-10\n",
    "vgg11 = models.vgg11(pretrained=True)\n",
    "vgg11.classifier[6] = nn.Linear(4096, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f87fe16-2108-4775-a28a-d0419ea45cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model 4: Feature Extraction + SVM\n",
    "\n",
    "def extract_features(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    feats, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            x = model.features(images)\n",
    "            x = torch.flatten(x, 1)\n",
    "            feats.append(x.cpu().numpy())\n",
    "            labs.append(labels.numpy())\n",
    "    return np.concatenate(feats), np.concatenate(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d546f-f291-4b8d-9a03-ef070dfcad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LeNet...\n",
      "Epoch [1/10], Loss: 0.2356\n",
      "Epoch [2/10], Loss: 0.0697\n",
      "Epoch [3/10], Loss: 0.0506\n",
      "Epoch [4/10], Loss: 0.0394\n",
      "Epoch [5/10], Loss: 0.0324\n",
      "Epoch [6/10], Loss: 0.0269\n",
      "Epoch [7/10], Loss: 0.0238\n",
      "Epoch [8/10], Loss: 0.0199\n",
      "Epoch [9/10], Loss: 0.0174\n",
      "Epoch [10/10], Loss: 0.0155\n",
      "Evaluating LeNet...\n",
      "Accuracy: 0.9856\n",
      "Confusion Matrix:\n",
      " [[ 977    1    0    0    0    0    1    0    0    1]\n",
      " [   1 1117    0    3    0    2    8    2    2    0]\n",
      " [   0    1 1019    7    0    0    1    3    1    0]\n",
      " [   1    1    0 1005    0    1    0    0    0    2]\n",
      " [   0    0    0    0  965    0    6    0    2    9]\n",
      " [   0    0    0   13    0  871    3    0    0    5]\n",
      " [   4    1    0    0    1    2  950    0    0    0]\n",
      " [   0    6    4    1    1    1    0  996    1   18]\n",
      " [   2    0    2    2    0    0    1    0  959    8]\n",
      " [   1    2    0    1    6    1    0    1    0  997]]\n",
      "Training Improved LeNet...\n",
      "Epoch [1/10], Loss: 0.3634\n",
      "Epoch [2/10], Loss: 0.1237\n",
      "Epoch [3/10], Loss: 0.1031\n",
      "Epoch [4/10], Loss: 0.0871\n",
      "Epoch [5/10], Loss: 0.0769\n",
      "Epoch [6/10], Loss: 0.0714\n",
      "Epoch [7/10], Loss: 0.0645\n",
      "Epoch [8/10], Loss: 0.0609\n",
      "Epoch [9/10], Loss: 0.0555\n",
      "Epoch [10/10], Loss: 0.0527\n",
      "Evaluating Improved LeNet...\n",
      "Accuracy: 0.9919\n",
      "Confusion Matrix:\n",
      " [[ 975    1    0    0    0    0    1    1    1    1]\n",
      " [   0 1133    0    1    0    0    0    1    0    0]\n",
      " [   1    2 1024    0    1    0    0    2    2    0]\n",
      " [   0    0    0 1001    0    2    0    3    3    1]\n",
      " [   0    1    0    0  975    0    0    1    0    5]\n",
      " [   0    0    0    6    0  885    1    0    0    0]\n",
      " [   2    3    0    1    2    1  948    0    1    0]\n",
      " [   0    2    3    0    0    0    0 1021    1    1]\n",
      " [   0    0    2    2    1    1    0    0  962    6]\n",
      " [   1    0    0    0    4    4    0    5    0  995]]\n",
      "Training Pretrained VGG11 on CIFAR-10...\n"
     ]
    }
   ],
   "source": [
    "# 6. Main Experiment\n",
    "if __name__ == '__main__':\n",
    "    # LeNet\n",
    "    lenet = LeNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(lenet.parameters(), lr=0.001)\n",
    "    print('Training LeNet...')\n",
    "    train_model(lenet, loader_train_mnist, criterion, optimizer, num_epochs=10)\n",
    "    print('Evaluating LeNet...')\n",
    "    evaluate_model(lenet, loader_test_mnist)\n",
    "\n",
    "    # Improved LeNet\n",
    "    imp_lenet = ImprovedLeNet()\n",
    "    optimizer2 = optim.Adam(imp_lenet.parameters(), lr=0.001)\n",
    "    print('Training Improved LeNet...')\n",
    "    train_model(imp_lenet, loader_train_mnist, criterion, optimizer2, num_epochs=10)\n",
    "    print('Evaluating Improved LeNet...')\n",
    "    evaluate_model(imp_lenet, loader_test_mnist)\n",
    "\n",
    "    # Pretrained VGG11\n",
    "    vgg = vgg11.to(device)\n",
    "    opt_vgg = optim.Adam(vgg.parameters(), lr=0.0001)\n",
    "    print('Training Pretrained VGG11 on CIFAR-10...')\n",
    "    train_model(vgg, loader_train_cifar, criterion, opt_vgg, num_epochs=5)\n",
    "    print('Evaluating VGG11...')\n",
    "    evaluate_model(vgg, loader_test_cifar)\n",
    "\n",
    "    # Features + SVM (using LeNet)\n",
    "    print('Extracting features from LeNet...')\n",
    "    X_train, y_train = extract_features(lenet, loader_train_mnist)\n",
    "    X_test, y_test = extract_features(lenet, loader_test_mnist)\n",
    "    np.save('features_train.npy', X_train)\n",
    "    np.save('labels_train.npy', y_train)\n",
    "    np.save('features_test.npy', X_test)\n",
    "    np.save('labels_test.npy', y_test)\n",
    "    print('Training SVM on extracted features...')\n",
    "    svc = svm.SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred_svm = svc.predict(X_test)\n",
    "    print('SVM Accuracy:', accuracy_score(y_test, y_pred_svm))\n",
    "\n",
    "    # Full CNN comparison complete\n",
    "    print('All experiments completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7aa9a4-228a-4468-9dfd-19086b798b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
